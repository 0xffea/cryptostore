# Cryptostore sample config file

# Redis or Kafka are required. They are used to batch updates from cryptofeed and the storage medium of choice
#
# del_after_read: (redis only) toggles the removal of data from redis after it has been processed with cryptostore.
# start_flush: toggles if redis/kafka should be flushed at the start. Primarily for debugging, it will flush ALL of redis/kafka
cache: redis

kafka:
    # ip/port are for the bootstrap server
    ip: '127.0.0.1'
    port: 9092
    start_flush: true
redis:
    ip: '127.0.0.1'
    port: 6379
    del_after_read: true
    start_flush: true

# Data sources and data types configured under exchanges. Exchange names follow the naming scheme in cryptofeed (they
# must be capitalized) and only exchanges supported by cryptofeed are supported.
# data types follow cryptofeed definitions, see defines.py in cryptofeed for more details, common ones are
# trades, l2_book, l3_book, funding, ticker (only trades, l2_book, l3_book currently supported by cryptostore).
# Trading pairs for all exchanges (except BitMEX) follow the currency-quote format 
exchanges:
    BITMEX:
        l2_book: [XBTUSD]

# number of levels per side to store from book updates. Remove this option to store all available data
book_depth: 10

# Where to store the data. Currently arctic and parquet are supported
storage: [arctic]


# Parquet specific options. Parquet will default to storing the data on disk unless these are specified
parquet:
    # if storing the data to an external source (like S3) toggle this to enable the removal of the local file after
    # writing to external store
    del_file: true

    S3:
        # If NULL boto will default to using ENV vars or credentials file
        # prefix: a prefix to append to the default data path
        key_id: null
        secret: null
        bucket: null
        prefix: null
    GCS:
        # path to service account key, if null will default to using env vars or auth tokens
        # on GCE node
        # prefix: a prefix to append to the default data path
        service_account: null
        bucket: null
        prefix: null

# arctic specific configuration options - the mongo URL
arctic: mongodb://127.0.0.1

# Data batching window, in seconds
storage_interval: 10
